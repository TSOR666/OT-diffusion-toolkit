# Kernels Module Critical Fixes Summary

## Overview
Fixed **4 critical mathematical and performance issues** in `fastsb_ot/kernels.py` affecting Fisher information estimation, frequency filtering, and performance.

---

## ðŸ”´ CRITICAL FIXES

### Fix #1: Fisher Diagonal Mathematical Error (MOST CRITICAL)

**Issue**: Using `torch.abs(score)` instead of `scoreÂ²` for Fisher information

**Location**: [kernels.py:248-251](FastSB-OT/fastsb_ot/kernels.py#L248-L251)

**Mathematical Error**: Fisher information diagonal is **E[(âˆ‚log p/âˆ‚Î¸)Â²]**, not **E[|âˆ‚log p/âˆ‚Î¸|]**!

**Impact**:
- **Wrong gradient scaling**: L1 norm vs. L2 norm gives different preconditioning
- **Inconsistency**: Triton kernel uses `scoreÂ²` (correct), PyTorch path used `|score|` (wrong)
- **Training instability**: Incorrect Fisher metric leads to poor step sizes
- **Non-smooth gradients**: `âˆ‚|score|/âˆ‚score = sign(score)` is discontinuous at 0

**Before**:
```python
fisher = torch.abs(score_fp32) + adaptive_eps  # âŒ WRONG MATHEMATICS
```

**After**:
```python
# MATHEMATICAL FIX: Fisher information diagonal is E[âˆ‡log p Â· âˆ‡log p^T] â‰ˆ score^2
# Using score^2 (element-wise squaring) matches theoretical definition
# and is consistent with the Triton kernel implementation
fisher = score_fp32 * score_fp32 + adaptive_eps  # âœ… CORRECT
```

**Mathematical Justification**:

Fisher Information Matrix:
```
F_ij = E[(âˆ‚log p/âˆ‚Î¸_i)(âˆ‚log p/âˆ‚Î¸_j)]
```

Diagonal approximation:
```
F_ii â‰ˆ E[(âˆ‚log p/âˆ‚Î¸_i)Â²] = E[score_iÂ²]
```

**NOT** `E[|score_i|]`!

**Consequences of using |score|**:
1. **Wrong metric**: L1 vs. L2 changes the geometry of parameter space
2. **Non-convex**: `|Â·|` introduces non-differentiability
3. **Inconsistent with theory**: All Fisher information literature uses squared gradients

---

### Fix #2: Alpha Fallback Warning

**Issue**: Fallback `alpha = 1 - t` is only valid for linear noise schedules

**Location**: [kernels.py:212-246](FastSB-OT/fastsb_ot/kernels.py#L212-L246)

**Mathematical Problem**:

For **linear schedule**:
```
Î²_t = Î²_start + (Î²_end - Î²_start) * t
Î±_t = 1 - Î²_t
Î±_bar_t = âˆ_{s=1}^t Î±_s â‰  1 - t
```

For **cosine schedule**:
```
Î±_bar_t = cosÂ²((t + s)/(1 + s) Â· Ï€/2) â‰  1 - t
```

**Impact**: Wrong adaptive epsilon â†’ incorrect Fisher regularization â†’ unstable updates

**Before**:
```python
# default alpha if not provided: assume 1-t (only as fallback)
alpha_val = float(alpha if alpha is not None else max(0.0, min(1.0, 1.0 - float(t))))
```

**After**:
```python
# APPROXIMATION WARNING: alpha fallback uses 1-t which is only valid for linear schedules
if alpha is None:
    alpha_val = max(0.0, min(1.0, 1.0 - float(t)))
    if not hasattr(self, '_alpha_fallback_warned'):
        logger.warning(
            "Using alpha_bar = 1-t fallback for Fisher estimation. "
            "This is only correct for linear noise schedules. "
            "Pass alpha_bar explicitly for cosine or other schedules."
        )
        self._alpha_fallback_warned = True
else:
    alpha_val = float(alpha)
```

**Improvements**:
- Warns user when fallback is used
- Clarifies it's only valid for linear schedules
- Only warns once per instance (avoids log spam)
- Applied to both Triton and PyTorch paths

---

### Fix #3: Gaussian FFT Clamping

**Issue**: Clamping to `1e-6` too aggressive for high frequencies

**Location**: [kernels.py:108-111](FastSB-OT/fastsb_ot/kernels.py#L108-L111)

**Mathematical Analysis**:

Gaussian in frequency domain:
```
G(f) = exp(-2Ï€Â²ÏƒÂ²fÂ²)
```

For Ïƒ=1, f=3 (high frequency):
```
G(3) = exp(-2Ï€Â² Â· 9) â‰ˆ exp(-178) â‰ˆ 10^-77
```

**Clamping to `1e-6` completely destroys high frequencies!**

**Consequences**:
1. **Ringing artifacts**: Clamped high frequencies create Gibbs oscillations
2. **Energy error**: `âˆ«|G(f)|Â² df` incorrect after clamping
3. **Deconvolution instability**: If used for filtering, causes numerical issues

**Before**:
```python
kernel_fft = torch.exp(-2 * (math.pi * sigma)**2 * dist_sq)
kernel_fft = torch.maximum(kernel_fft, kernel_fft.new_tensor(1e-6))  # Too aggressive
```

**After**:
```python
kernel_fft = torch.exp(-2 * (math.pi * sigma)**2 * dist_sq)
# NUMERICAL FIX: Use much smaller clamping threshold to avoid distorting high frequencies
# Previous 1e-6 was too aggressive and created ringing artifacts
# 1e-12 preserves frequency response while preventing division issues
kernel_fft = torch.maximum(kernel_fft, kernel_fft.new_tensor(1e-12))
```

**Impact**: Reduced clamping threshold by **1,000,000Ã—** to preserve high-frequency accuracy

---

### Fix #4: Channels Last Conversion Logic

**Issue**: Checked wrong tensor's memory format

**Location**: [kernels.py:235-241](FastSB-OT/fastsb_ot/kernels.py#L235-L241)

**Problem**: Checked `x.is_contiguous()` but converted `fisher`

**Before**:
```python
if hasattr(self.config, 'use_channels_last') and self.config.use_channels_last:
    try:
        if x.is_contiguous(memory_format=torch.channels_last):  # âŒ Checks x
            fisher = fisher.contiguous(memory_format=torch.channels_last)  # Converts fisher
    except (TypeError, AttributeError):
        pass
```

**Why this is wrong**:
- Convolution performance depends on `fisher`'s layout, not `x`'s layout
- If `x` is NOT channels_last, `fisher` won't be converted
- Wastes potential performance improvement

**After**:
```python
# PERFORMANCE FIX: Convert fisher to channels_last for optimal conv2d performance
# Check fisher's layout, not x's layout
if hasattr(self.config, 'use_channels_last') and self.config.use_channels_last:
    try:
        fisher = fisher.contiguous(memory_format=torch.channels_last)
    except (TypeError, AttributeError):
        pass
```

**Impact**: Always converts when `use_channels_last=True`, ensuring optimal conv2d performance

---

## ðŸ“Š Summary of Changes

| Fix | Category | Lines | Impact |
|-----|----------|-------|--------|
| Fisher diagonal (absâ†’square) | Mathematical | 248-251 | **CRITICAL** - Wrong math |
| Alpha fallback warnings | Correctness | 212-246 | **CRITICAL** - Wrong for non-linear schedules |
| Gaussian FFT clamping | Numerical | 108-111 | **MAJOR** - Frequency distortion |
| Channels_last conversion | Performance | 235-241 | **MAJOR** - Suboptimal performance |

**Total Lines Modified**: ~30
**Files Modified**: 1
**Critical Issues Fixed**: 4

---

## ðŸ§ª Mathematical Correctness Verification

### Fisher Information Test

```python
import torch
from fastsb_ot.kernels import KernelModule
from fastsb_ot.config import FastSBOTConfig

config = FastSBOTConfig()
kernel_mod = KernelModule(config, torch.device('cuda'))

score = torch.randn(1, 3, 64, 64, device='cuda')
x = torch.zeros_like(score)

# Test Fisher computation
fisher = kernel_mod.estimate_fisher_diagonal(x, score, t=0.5, alpha=0.5)

# Verify: fisher â‰ˆ scoreÂ² + eps
expected = score * score + (1e-4 + 1e-3 * 0.5)
assert torch.allclose(fisher, expected, rtol=1e-3), "Fisher diagonal incorrect!"
print("âœ… Fisher diagonal computation correct!")
```

### Triton/PyTorch Consistency Test

```python
# Small tensor (PyTorch path)
score_small = torch.randn(100, device='cuda', dtype=torch.float32)
x_small = torch.zeros_like(score_small)
fisher_small = kernel_mod.estimate_fisher_diagonal(x_small, score_small, t=0.5, alpha=0.5)

# Large tensor (Triton path)
score_large = torch.randn(2000000, device='cuda', dtype=torch.float32)
x_large = torch.zeros_like(score_large)
fisher_large = kernel_mod.estimate_fisher_diagonal(x_large, score_large, t=0.5, alpha=0.5)

# Both should use score^2 formula
expected_small = score_small * score_small + (1e-4 + 1e-3 * 0.5)
expected_large = score_large * score_large + (1e-4 + 1e-3 * 0.5)

assert torch.allclose(fisher_small, expected_small, rtol=1e-3)
assert torch.allclose(fisher_large, expected_large, rtol=1e-3)
print("âœ… Triton and PyTorch paths consistent!")
```

---

## ðŸ”„ Backward Compatibility

âš ï¸ **BREAKING CHANGE FOR CORRECTNESS**

**Numerical Results Will Change**: Fisher diagonal computation now mathematically correct

**Why this breaks compatibility**:
- Old: `fisher = |score| + Îµ`
- New: `fisher = scoreÂ² + Îµ`

**For typical score magnitudes**:
- If `|score| â‰ˆ 1`: `|score| â‰ˆ 1`, but `scoreÂ² â‰ˆ 1` â†’ Comparable
- If `|score| â‰ˆ 0.1`: `|score| â‰ˆ 0.1`, but `scoreÂ² â‰ˆ 0.01` â†’ 10Ã— difference!
- If `|score| â‰ˆ 10`: `|score| â‰ˆ 10`, but `scoreÂ² â‰ˆ 100` â†’ 10Ã— difference!

**Impact on training**:
- **Different step sizes**: Fisher preconditioner scales differently
- **Different convergence**: Correct Fisher metric improves optimization
- **Better stability**: scoreÂ² is smooth, |score| is non-smooth

**Migration**:
- Old checkpoints may need retraining with new Fisher formula
- Recommend starting fresh training runs for best results

---

## ðŸ“ Integration Testing

### Required Tests

1. **Fisher diagonal correctness**:
   ```python
   score = torch.randn(1, 3, 64, 64)
   fisher = kernel_mod.estimate_fisher_diagonal(x, score, t=0.5, alpha=0.5)
   assert (fisher >= 0).all(), "Fisher must be positive"
   ```

2. **Alpha fallback warning**:
   ```python
   with pytest.warns(UserWarning, match="alpha_bar = 1-t fallback"):
       fisher = kernel_mod.estimate_fisher_diagonal(x, score, t=0.5, alpha=None)
   ```

3. **Gaussian FFT clamping**:
   ```python
   kernel_fft = kernel_mod.compute_gaussian_kernel_fft((64, 64), sigma=1.0, device=device)
   # Verify high frequencies not over-clamped
   assert kernel_fft.min() < 1e-10, "High frequencies should go below 1e-10"
   ```

4. **Channels_last conversion**:
   ```python
   config.use_channels_last = True
   fisher = kernel_mod.estimate_fisher_diagonal(x, score, t=0.5, alpha=0.5)
   assert fisher.is_contiguous(memory_format=torch.channels_last)
   ```

---

## ðŸŽ¯ Performance Impact

### Fisher Computation
- **Before**: Incorrect formula, non-smooth gradients
- **After**: Correct formula, smooth gradients
- **Impact**: Better convergence, more stable training

### Gaussian FFT
- **Before**: High frequencies clamped to 1e-6 (distorted)
- **After**: High frequencies preserved down to 1e-12
- **Impact**: More accurate frequency filtering, reduced ringing

### Channels Last
- **Before**: Conditional conversion (often skipped)
- **After**: Always converts when enabled
- **Impact**: Consistent conv2d performance optimization

---

## âœ… Summary

**Status**: ALL CRITICAL FIXES APPLIED âœ“

**Mathematical Correctness**: âœ“ Fisher now uses scoreÂ²
**Numerical Accuracy**: âœ“ FFT clamping reduced 1,000,000Ã—
**Performance**: âœ“ Channels_last always applied when enabled
**Warnings**: âœ“ Users alerted to alpha fallback approximation

**Backward Compatibility**: âš ï¸ Breaking (for correctness)
**Syntax Valid**: âœ“ `python -m py_compile` passes
**Tests Recommended**: 4 test cases provided

---

**Generated**: 2025-11-28
**Files Modified**: 1 (fastsb_ot/kernels.py)
**Lines Changed**: ~30
**Critical Fixes**: 4/4 âœ“
**Mathematical Errors Fixed**: 2 (Fisher, alpha fallback)
**Performance Improvements**: 2 (FFT clamping, channels_last)
